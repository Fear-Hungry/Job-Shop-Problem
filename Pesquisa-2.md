## Resultados da Pesquisa
A que obteve o melhor desempenho foi o DRL-Assisted Adaptive Genetic Algorithm (DRL-A-GA) proposto por Ma et al. (2025), superando tanto algoritmos clássicos quanto outros híbridos em instâncias-benchmark, com makespan menor e convergência mais rápida

# Hibridização de DRL e Metaheurísticas para o JSP

O **Problema de Escalonamento em Job Shop (JSP)** é NP-difícil e tradicionalmente enfrentado por heurísticas ou metaheurísticas. Abordagens recentes empregam *Deep Reinforcement Learning* (DRL) para aprender políticas de escalonamento que geram soluções de alta qualidade ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=computational%20components%20including%20conventional%20neural,global%20optimal%20solutions)). Estudos demonstram que métodos DRL superam solvers exatos e heurísticas simples em velocidade computacional e qualidade de solução quase ótima ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=computational%20components%20including%20conventional%20neural,global%20optimal%20solutions)). Entretanto, o DRL isolado enfrenta limitações em escalabilidade, generalização e interpretação ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=environments%20with%20different%20job%20characteristics,This%20paper%20serves)). Por isso, pesquisas atuais investigam **abordagens híbridas**, em que modelos de DRL e metaheurísticas colaboram ou rodam em paralelo, combinando a aprendizagem adaptativa do DRL com a busca global dos meta-heurísticos.

## Principais Metaheurísticas Híbridas com DRL

Diversas metaheurísticas de estado da arte vêm sendo integradas a DRL para resolver o JSP (monoobjetivo, e.g., minimizar *makespan*). Entre elas destacam-se:

- **Algoritmos Genéticos (GA):** Em hibridizações *GA+DRL*, o agente DRL aprende a ajustar parâmetros do GA ou a escolher operadores genéticos. Por exemplo, Chien e Lan (2021) propuseram um agente DQN (Deep Q-Network) híbrido com GA para escalonamento em múltiplas máquinas em paralelo. Nesse modelo, o agente RL com estratégia ε-greedy atua como controlador do GA, resultando em *makespan* menor e tempo de decisão rápido diante de mudanças dinâmicas ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=energy%20saving%20and%20total%20makespan,shortage%20in%20an%20unrelated%20parallel)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=The%20Kalman%20filter%20and%20the,world%20scenarios%20than%20other%20solutions)). Mais recentemente, Ma et al. (2025) introduziram um *DRL-Assisted Adaptive GA* para o JSP flexível, onde o estado do GA (população) é alimentado a um DRL que escolhe entre mutações específicas e ajusta parâmetros-chave ([
        A deep reinforcement learning assisted adaptive genetic algorithm for flexible job shop scheduling
      -  Georgia Southern University](https://scholars.georgiasouthern.edu/en/publications/a-deep-reinforcement-learning-assisted-adaptive-genetic-algorithm#:~:text=efficiently%2C%20which%20losses%20the%20optimality,benchmark%20instances%20are%20selected%20for)). Os resultados mostram que esse DRL-A-GA supera algoritmos clássicos e híbridos anteriores, melhorando a qualidade das soluções e acelerando a convergência ([
        A deep reinforcement learning assisted adaptive genetic algorithm for flexible job shop scheduling
      -  Georgia Southern University](https://scholars.georgiasouthern.edu/en/publications/a-deep-reinforcement-learning-assisted-adaptive-genetic-algorithm#:~:text=GA%20and%20dynamically%20select%20appropriate,algorithms%20for%20solving%20FJSP%2C%20effectively)). **Vantagem:** combina exploração global do GA com políticas aprendidas, elevando a qualidade dos horários ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=The%20Kalman%20filter%20and%20the,world%20scenarios%20than%20other%20solutions)) ([
        A deep reinforcement learning assisted adaptive genetic algorithm for flexible job shop scheduling
      -  Georgia Southern University](https://scholars.georgiasouthern.edu/en/publications/a-deep-reinforcement-learning-assisted-adaptive-genetic-algorithm#:~:text=GA%20and%20dynamically%20select%20appropriate,algorithms%20for%20solving%20FJSP%2C%20effectively)). **Desvantagem:** aumento da complexidade computacional (treinamento do agente + execução do GA) e necessidade de grande poder computacional para treinar o DRL.

- **Otimização por Enxame (p.ex. Grey Wolf, PSO):** Algoritmos inspirados em enxames também têm sido alvo de hibridização. Yin et al. (2020) integraram um agente DRL a um *Grey Wolf Optimizer* (GWO) para um problema de flow shop com objetivos de makespan e consumo de energia. O DRL adapta as subpopulações do GWO online, usando ainda um filtro de Kalman para convergir à frente de Pareto ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Yin%20et%C2%A0al,designed%20a%20hybrid)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Yin%20et%C2%A0al,search%20through%20the%20solution%20space)). Analogamente, o *Particle Swarm Optimization* (PSO) poderia ser combinado com DRL ajustando coeficientes de aprendizado das partículas. **Vantagem:** esses métodos mantêm diversidade (vários agentes) e o DRL pode guiar efetivamente a busca global ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Yin%20et%C2%A0al,search%20through%20the%20solution%20space)). **Desvantagem:** muitos parâmetros para ajustar (ex.: tamanho de enxame, coeficientes), além de alto custo de execução em problemas grandes.

- **Estimação de Distribuição (EDA) + DRL:** Du et al. (2022) propuseram um esquema híbrido EDA+DQN para JSP flexível. O EDA realiza exploração de larga escala (modelando a distribuição de soluções) enquanto um DQN aprende a refinar soluções promissoras. Em cada iteração, o EDA amplia a busca sobre o espaço de soluções para evitar ótimos locais, e o DQN seleciona movimentos locais (buscas por vizinhança) apropriados para melhorar a solução atual ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=machine%20scheduling%20setting,search%20method%20for%20specific%20scheduling)). Essa combinação aproveita a capacidade exploratória do EDA e a de exploração local do DRL; na prática, superou tanto heurísticas metaheurísticas isoladas quanto um DQN puro ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Du%20et%C2%A0al,the%20Sim2Real%20gap%20through%20continuous)). **Vantagem:** alto equilíbrio exploração-exploração e melhor cobertura do espaço de solução ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Du%20et%C2%A0al,the%20Sim2Real%20gap%20through%20continuous)). **Desvantagem:** complexidade na manutenção da distribuição estatística do EDA, além do overhead de treinar/rodar o agente DQN em cada iteração.

- **Programação Genética (GP) + DRL (Hiper-heurística):** Li et al. (2022) empregaram GP para evoluir uma combinação de regras de despacho (prioridade de operações) como espaço de ações para um agente DRL. Ou seja, o GP otimiza pesos de várias regras simples, e as melhores combinações geradas tornam-se ações que o agente DQN pode escolher em tempo de execução. Assim, o agente aprende a selecionar heurísticas compostas de forma adaptativa. Esse híbrido GP+DQN mostrou-se robusto a distúrbios dinâmicos (como quebra de máquina ou jobs urgentes) e gerou políticas real-time mais eficazes ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=which%20chose%20an%20appropriate%20local,generated%20actions%2C%20hence%20improving%20the)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=problems%20bigger%20than%2010%20jobs,the%20Sim2Real%20gap%20through%20continuous)). **Vantagem:** pode criar heurísticas de despacho poderosas e adaptativas, melhorando a flexibilidade e qualidade das soluções em ambientes variáveis ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=problems%20bigger%20than%2010%20jobs,the%20Sim2Real%20gap%20through%20continuous)). **Desvantagem:** o processo de evolução inicial (GP) pode ser caro e dependente de boas regras de base, além de aumentar ainda mais o custo computacional.

- **Busca por Vizinhança (LNS/ALNS) + DRL:** Heurísticas como *Large Neighborhood Search* (LNS) ou *Adaptive LNS* empregam operadores de destruição/reparo para melhorar soluções. Uma possível estratégia híbrida é usar RL para selecionar quais operadores aplicar em cada passo. Embora ainda rara em JSP, abordagens afins foram testadas em problemas de logística: Zhang et al. (2023) apresentaram um algoritmo híbrido ACO+LNS onde um agente de RL decide quais operadores de vizinhança usar para um problema de agendamento de caminhões ([bohrium.dp.tech](https://bohrium.dp.tech/paper/arxiv/e4a4b55928f78747be898fcbb20f4736e71de29fa6cad400aaeb5b5981ce205e#:~:text=,21%20Abstract%3AThe)). Em contexto similar, DRL pode adaptar online a escolha de vizinhanças em busca de soluções melhores. **Vantagem:** alta flexibilidade e potencial de solução de ótima qualidade, explorando vizinhanças grandes de forma inteligente. **Desvantagem:** requer configurar múltiplos operadores e o agente RL adiciona latência; implementação e ajuste fino desses componentes é complexa.

- **Outras Metaheurísticas:** Algoritmos clássicos adicionais podem ser combinados. Por exemplo, em um híbrido DRL+ACO, o agente RL poderia ajustar níveis de feromônio ou escolher rotas promissoras. Em DRL+SA, RL poderia controlar a temperatura de resfriamento ou critério de aceitação. Tabu Search também pode ser guiado por RL para ajustar a lista tabu dinamicamente. Embora a literatura recente tenha poucos estudos diretos em JSP para estes casos, eles são candidatos naturais de hibridização. **Vantagens genéricas:** cada metaheurística aporta mecanismos únicos (e.g. diversificação de SA, memória de TS, cooperação de ACO) que, combinados com aprendizagem RL, podem superar abordagens convencionais. **Desvantagens genéricas:** pouca evidência consolidada em JSP, dificuldade de implementar e verificar rigorosamente, além do custo computacional extra.

## Estratégias de Hibridização

As pesquisas identificam várias estratégias de integração DRL+meta-heurísticas:

- **Inicialização e Refinamento:** Um agente DRL constrói uma solução inicial (ou pool de soluções), e uma meta-heurística (busca local, SA, etc.) a refina iterativamente. Isso aproveita a capacidade do DRL de gerar boas soluções rapidamente, seguidas de melhora local efetiva.

- **Guia de Heurísticas:** O DRL atua como *hiper-heurística*, aprendendo a escolher operadores ou regras de despacho durante a busca. Por exemplo, o DQN de Du et al. é usado como *seletor de busca local*, decidindo qual vizinhança aplicar em cada estado ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=component%20could%20investigate%20a%20bigger,search%20method%20for%20specific%20scheduling)). Similarmente, um agente pode aprender a selecionar operadores de mutação no GA ou quais movimentos tentar no LNS.

- **Ajuste de Parâmetros:** O agente DRL pode ajustar dinamicamente parâmetros de execução dos meta-heurísticos. No modelo DRL-A-GA de Ma et al., por exemplo, o estado representa estatísticas da população e o agente decide sobre taxas de mutação ou que operação genética aplicar ([
        A deep reinforcement learning assisted adaptive genetic algorithm for flexible job shop scheduling
      -  Georgia Southern University](https://scholars.georgiasouthern.edu/en/publications/a-deep-reinforcement-learning-assisted-adaptive-genetic-algorithm#:~:text=efficiently%2C%20which%20losses%20the%20optimality,benchmark%20instances%20are%20selected%20for)). Assim, o DRL otimiza a própria execução do meta-heurístico.

- **Soluções Paralelas:** Duas (ou mais) técnicas podem rodar em paralelo. Por exemplo, o GA evolui uma população enquanto um agente RL tenta melhorar a melhor solução encontrada (ou vice-versa). Alternativamente, múltiplos agentes cooperativos podem atuar em partes diferentes do problema, trocando informações.

- **Evolução de Espaço de Ações:** Metodologias de programação genética ou construção de regras criam um espaço de ações robusto para o DRL. Como no caso de Li et al., o GP evolui regras de despacho e o DRL as usa como ações disponíveis ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=which%20chose%20an%20appropriate%20local,generated%20actions%2C%20hence%20improving%20the)). Essa estratégia amplia a capacidade do agente de generalizar e melhorar soluções.

## Vantagens e Desvantagens

Em geral, **híbridos DRL+metaheurísticos** tendem a superar técnicas tradicionais isoladas: ao combinar forças, eles podem encontrar soluções de melhor qualidade mais rápido. Estudos relatam que, por exemplo, métodos híbridos DQN–GA obtêm *makespan* menores e convergem mais rápido que GA puro ou DQN puros ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=The%20Kalman%20filter%20and%20the,world%20scenarios%20than%20other%20solutions)) ([
        A deep reinforcement learning assisted adaptive genetic algorithm for flexible job shop scheduling
      -  Georgia Southern University](https://scholars.georgiasouthern.edu/en/publications/a-deep-reinforcement-learning-assisted-adaptive-genetic-algorithm#:~:text=GA%20and%20dynamically%20select%20appropriate,algorithms%20for%20solving%20FJSP%2C%20effectively)). Similarmente, EDA+DQN demonstrou superar tanto metaheurísticas isoladas quanto abordagens DRL simples ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Du%20et%C2%A0al,the%20Sim2Real%20gap%20through%20continuous)). De modo geral, os sistemas híbridos conseguem uma busca mais ampla e adaptativa, equilibrando exploração e exploração. Por outro lado, trazem **complexidade**: exige definir estados, ações e recompensas adequadas, treinar agentes (o que pode demandar muitos episódios) e integrar sistemas heterogêneos. Conforme ressalta Khadivi et al., métodos DRL – e por extensão híbridos – enfrentam desafios de escalabilidade, interpretabilidade e robustez a cenários não vistos ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=environments%20with%20different%20job%20characteristics,This%20paper%20serves)). A implementação híbrida geralmente é custosa em termos de tempo de desenvolvimento e recursos computacionais. Em resumo, há um trade-off: ganhos em desempenho costumam vir acompanhados de maior complexidade computacional e necessidade de ajuste fino.

## Resumo Comparativo

| Metaheurística         | Integração com DRL                                          | Exemplo/Implementação (Ano)                                | Vantagens Principais                          | Desvantagens Principais                               |
|------------------------|-------------------------------------------------------------|-----------------------------------------------------------|-----------------------------------------------|------------------------------------------------------|
| **Algoritmo Genético (GA)**    | DRL ajusta parâmetros do GA ou escolhe operadores genéticos  | Chien & Lan (2021) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=energy%20saving%20and%20total%20makespan,shortage%20in%20an%20unrelated%20parallel)); Ma et al. (2025) ([
        A deep reinforcement learning assisted adaptive genetic algorithm for flexible job shop scheduling
      -  Georgia Southern University](https://scholars.georgiasouthern.edu/en/publications/a-deep-reinforcement-learning-assisted-adaptive-genetic-algorithm#:~:text=efficiently%2C%20which%20losses%20the%20optimality,benchmark%20instances%20are%20selected%20for)) | Melhora qualidade do makespan; convergência rápida ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=The%20Kalman%20filter%20and%20the,world%20scenarios%20than%20other%20solutions)) ([
        A deep reinforcement learning assisted adaptive genetic algorithm for flexible job shop scheduling
      -  Georgia Southern University](https://scholars.georgiasouthern.edu/en/publications/a-deep-reinforcement-learning-assisted-adaptive-genetic-algorithm#:~:text=GA%20and%20dynamically%20select%20appropriate,algorithms%20for%20solving%20FJSP%2C%20effectively)) | Treinamento pesado; muitas operações híbridas (GA + DRL) |
| **Otimização por Enxame** (e.g. GWO) | DRL modifica população/coeficientes de partículas    | Yin et al. (2020) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Yin%20et%C2%A0al,designed%20a%20hybrid)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Yin%20et%C2%A0al,search%20through%20the%20solution%20space))            | Boa diversificação; busca global eficiente     | Vários hiperparâmetros; custo elevado                  |
| **Estimação de Distribuição (EDA)** | DRL seleciona amostragens e busca local             | Du et al. (2022) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=machine%20scheduling%20setting,search%20method%20for%20specific%20scheduling)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Du%20et%C2%A0al,was%20not%20able%20to%20solve))            | Combinação de exploração ampla e refinamento local ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Du%20et%C2%A0al,the%20Sim2Real%20gap%20through%20continuous)) | Manutenção complexa de distribuição; computacionalmente pesado |
| **Programação Genética (GP)** | GP evolui regras; DRL escolhe regras para ação        | Li et al. (2022) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=which%20chose%20an%20appropriate%20local,generated%20actions%2C%20hence%20improving%20the)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=problems%20bigger%20than%2010%20jobs,the%20Sim2Real%20gap%20through%20continuous))            | Criação de regras adaptativas; robusto a distúrbios ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=problems%20bigger%20than%2010%20jobs,the%20Sim2Real%20gap%20through%20continuous)) | Processo de evolução caro; dependência de regras base |
| **LNS/ALNS** (Busca de Vizinhança) | DRL escolhe operadores de destruição/reparo          | Zhang et al. (2023) (contêiner) ([bohrium.dp.tech](https://bohrium.dp.tech/paper/arxiv/e4a4b55928f78747be898fcbb20f4736e71de29fa6cad400aaeb5b5981ce205e#:~:text=,21%20Abstract%3AThe)) (análogo)           | Alta qualidade de solução; flexível             | Configuração complexa; muito recursos computacionais   |
| **Outros (ACO, PSO, SA, TS, etc.)** | DRL guia o processo (p.ex. feromônios, temperatura) | (Ainda pouco explorado diretamente em JSP híbrido)          | Potencial de diversificação e aprendizado      | Falta de estudos consolidados; integração difícil      |

*Fontes:* Estudos recentes e revisões no campo ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=energy%20saving%20and%20total%20makespan,shortage%20in%20an%20unrelated%20parallel)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=machine%20scheduling%20setting,search%20method%20for%20specific%20scheduling)) ([[2310.03195] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions](https://ar5iv.org/pdf/2310.03195#:~:text=Du%20et%C2%A0al,the%20Sim2Real%20gap%20through%20continuous)).

Em síntese, as combinações **DRL + metaheurística** almejam o melhor de dois mundos: flexibilidade e aprendizado adaptativo do DRL, somados à capacidade exploratória e robustez dos meta-heurísticos. Essas abordagens híbridas têm apresentado resultados promissores no JSP monoobjetivo, embora exijam maior esforço em projeto e cálculo. A pesquisa continua avançando, com novos trabalhos testando diferentes algoritmos híbridos e refinando estratégias de integração.
